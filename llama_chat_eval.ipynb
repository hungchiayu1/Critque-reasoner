{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0f1245-ed21-41c4-9c55-b69aff41c61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c660947d44c44e58a18ca80a616c6ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.utils import logging\n",
    "from transformers import GenerationConfig\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model, TaskType,PeftModel\n",
    "model_name = 'google/flan-t5-xl'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9898ca1e-df9e-4e16-b8ff-432816411fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "critque_model = PeftModel.from_pretrained(model, \"trained_model/flan_t5_multi_task_orca1,3/checkpoint-3800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8a71af-876b-44b0-9831-86c9c03d17cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/cyhung/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2194a87-e53a-4bee-9f85-8dd5e77b2699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 19:57:15.122800: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-27 19:57:15.122831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-27 19:57:15.122844: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-27 19:57:15.126029: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10b991c191248c7be70ddee791af482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "access_token = ###\n",
    "\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "generator = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            load_in_8bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f04a8cec-740b-4cd5-9230-1f8f9952ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikienv, wrappers\n",
    "import re\n",
    "import requests\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "env = wikienv.WikiEnv()\n",
    "#env = wrappers.FeverWrapper(env, split=\"dev\")\n",
    "env = wrappers.LoggingWrapper(env)\n",
    "\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "\n",
    "    def __call__(self,term,top_sentence=12):\n",
    "\n",
    "        return self.search_term(term,top_sentence)\n",
    "        \n",
    "    def step(self,env, action):\n",
    "        attempts = 0\n",
    "        while attempts < 10:\n",
    "            try:\n",
    "                return env.step(action)\n",
    "            except requests.exceptions.Timeout:\n",
    "                attempts += 1\n",
    "    \n",
    "    def search_term(self,term,top_sentence=12,count= 0):\n",
    "        if count >=2:\n",
    "            print(\"Nothing found\")\n",
    "            return ''\n",
    "        action = f'Search[{term}]'\n",
    "        action[0].lower() + action[1:]\n",
    "        print(action)\n",
    "        \n",
    "        res = self.step(env, action[0].lower() + action[1:])[0] ## There might be some unknown unicode decoding error here\n",
    "        \n",
    "        if isinstance(res,str): ## Could not find but found similar term\n",
    "            match = re.search(r\"Similar: \\[(.*?)\\]\", res)\n",
    "            print(res)\n",
    "            if match:\n",
    "                list_str = match.group(1)\n",
    "                # Split the list string and get the first element\n",
    "                elements = re.findall(r\"'(.*?)'\", list_str)\n",
    "                if elements:\n",
    "                    first_element = elements[0]\n",
    "                    print(f\"Searching the most similar term {first_element} instead\")\n",
    "                    count+=1\n",
    "                    return self.search_term(first_element,top_sentence,count)\n",
    "                    \n",
    "        if isinstance(res,list):\n",
    "            if res[0].startswith('There were no results matching the query'): ## Nothing found\n",
    "                print(\"Nothing found\")\n",
    "                return ''\n",
    "            sentence = ' '.join(res[:top_sentence]).replace('\\xa0','')\n",
    "            context = ' '.join(sentence.split(' ')[:300]) ## Take max 300 words\n",
    "            return context\n",
    "        print(\"Unknown error\",res)\n",
    "        return ''\n",
    "\n",
    "\n",
    "generation_config_critque = GenerationConfig(\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            top_p=0.75,\n",
    "            top_k=40,\n",
    "            max_new_tokens=512)\n",
    "\n",
    "generation_config_generator = GenerationConfig(\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            top_p=0.75,\n",
    "            top_k=40,\n",
    "            max_new_tokens=512)\n",
    "\n",
    "class Critque:\n",
    "\n",
    "    def __init__(self,\n",
    "                 critque_model,\n",
    "                 retriever,\n",
    "                 t5_tokenizer,\n",
    "                 generation_config=generation_config_critque,\n",
    "                 device='cuda',\n",
    "                 max_steps=3):\n",
    "        \n",
    "        self.critque_model = critque_model\n",
    "        self.retriever = retriever\n",
    "        self.t5_tokenizer = t5_tokenizer\n",
    "        self.generation_config = generation_config\n",
    "        self.max_steps=max_steps\n",
    "        self.device = device\n",
    "        \n",
    "    def _extract_t5_for_search(self,sentence):\n",
    "        ## I need to ... SEARCH[TERM]\n",
    "        pattern = r'\\[([^\\]]+)\\]' ## Check for terms inside square brackets\n",
    "\n",
    "        match = re.search(pattern, sentence)\n",
    "        if match:\n",
    "        # Extract the term within square brackets\n",
    "            extracted_term = match.group(1)\n",
    "            return extracted_term\n",
    "        else:\n",
    "            return ''\n",
    "    def critque_step(self,query):\n",
    "        ## This step returns either Context: [retrieved context] or FEEDBACK: feedback\n",
    "\n",
    "        \n",
    "        \n",
    "        inputs = self.t5_tokenizer(query,return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.critque_model.generate(input_ids=input_ids,\n",
    "                                                 do_sample=True,\n",
    "                                                 generation_config=self.generation_config,\n",
    "                                                 return_dict_in_generate=True)\n",
    "            generated_output = output.sequences[0]\n",
    "            s = self.t5_tokenizer.decode(generated_output,skip_special_tokens=True)\n",
    "        \n",
    "        search = self._extract_t5_for_search(s) ## Check if retrieval is required. This returns empty string or retrieval term\n",
    "        \n",
    "        if len(search)>0:  ## retrieval required\n",
    "            context = self.retriever(search)\n",
    "\n",
    "            if len(context)>0:\n",
    "                self.retrieval_flag = True\n",
    "               \n",
    "                return f\"Context: {context}\"\n",
    "\n",
    "            return f\"Context: No context\"\n",
    "            \n",
    "        return s ### This is the feedback step\n",
    "\n",
    "class Generator:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 generator,\n",
    "                 generator_tokenizer,\n",
    "                 generation_config=generation_config_generator,\n",
    "                 device='cuda',\n",
    "                 max_steps=3,\n",
    "                 ):\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.generator_tokenizer = generator_tokenizer\n",
    "        self.generation_config = generation_config_generator\n",
    "        self.device = device\n",
    "       \n",
    "    def qa_step(self,query):\n",
    "    \n",
    "        chat_template = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an instruction following question answering model and your goal is to answer question as truthfully as you can. You will make use of the context as much as possible and make use of the hint if it is provided. Give a short and concise answer to every query.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{query}\"}]\n",
    "        \n",
    "        input_ids = self.generator_tokenizer.apply_chat_template(chat_template,return_tensors='pt').to(\"cuda\")\n",
    "        input_len = len(input_ids[0])\n",
    "        #print(len(input_ids))\n",
    "        with torch.no_grad():\n",
    "                output = self.generator.generate(input_ids=input_ids,\n",
    "                                                     do_sample=True,\n",
    "                                                     generation_config=self.generation_config,\n",
    "                                                     return_dict_in_generate=True)\n",
    "        \n",
    "                generated_output = output.sequences[0][input_len:]\n",
    "            \n",
    "        return self.generator_tokenizer.decode(generated_output,skip_special_tokens=True)\n",
    "\n",
    "class CritqueGen:\n",
    "\n",
    "    def __init__(self,critque,generator,max_step=3):\n",
    "        self.critque = critque\n",
    "        self.generator = generator\n",
    "        self.max_steps = max_step\n",
    "        self.traj = []\n",
    "        \n",
    "    def inital_step(self,query):\n",
    "        context = self.critque.critque_step(query)\n",
    "        \n",
    "            \n",
    "        query_with_context = f\"{context}\\nQuestion:{query}\"\n",
    "        self.traj.append({\"Context\":context,\"Query\":query})\n",
    "\n",
    "     \n",
    "    def feedback_step(self,do_feedback):\n",
    "        context,query = self.traj[0]['Context'],self.traj[0]['Query']\n",
    "        \n",
    "        if len(self.traj) == 1:  ## First step, no feedback has been provided\n",
    "            context_with_query = f\"{context}\\n{query}\"\n",
    "            reasoning = self.generator.qa_step(context_with_query)\n",
    "            context_with_query_reasoning = f\"{context}\\n{query}\\nReasoning:{reasoning}\"\n",
    "            \n",
    "        else: ## The previous reasoning step and feedback is the last element of the trajectory list\n",
    "            prev_step = self.traj[-1]\n",
    "            reasoning,feedback = prev_step[\"Reasoning\"],prev_step[\"Feedback\"]\n",
    "            context_query_feedback = f\"{context}\\n{query}\\nReasoning:{reasoning}\\n{feedback}\"\n",
    "            print(context_query_feedback)\n",
    "            reasoning = self.generator.qa_step(context_query_feedback)\n",
    "            context_with_query_reasoning = f\"{context}\\n{query}\\nReasoning:{reasoning}\"\n",
    "            \n",
    "\n",
    "        feedback = self.critque.critque_step(context_with_query_reasoning)\n",
    "        \n",
    "        self.traj.append({\"Reasoning\":reasoning,\"Feedback\":feedback})\n",
    "        if feedback == 'FEEDBACK: No HINT':\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "        \n",
    "    def multi_step(self,query):\n",
    "        self.traj = []\n",
    "        \n",
    "        self.inital_step(query)\n",
    "\n",
    "        for i in range(2):\n",
    "            end = self.feedback_step()\n",
    "            if end:\n",
    "                ans = self.traj[-1]['Reasoning']\n",
    "                print(ans)\n",
    "                return ans\n",
    "                \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ed7117-179e-4568-8e1a-770f367e42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = Retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da6d6aa-6429-43c2-8d3e-1c3ebdee0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = Generator(generator,generator_tokenizer)\n",
    "critque = Critque(critque_model,ret,tokenizer)\n",
    "critque_gen = CritqueGen(critque,qa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accc4b44-8f3c-4f8c-b279-e0892f7f7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the population of Singapore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "251cb043-dc9e-4d50-8e05-255cdc2e3028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search[Singapore]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' According to the context, the population of Singapore is approximately 5.68 million people, as of 2020.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critque_gen.multi_step(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa0de727-1737-4f46-8d37-8d6637c82af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Context': \"Context: Singapore (/ˈsɪŋ(ɡ)əpɔːr/ ⓘ SING-(g)ə-por), officially the Republic of Singapore, is an island country and city-state in maritime Southeast Asia. It is located about one degree of latitude (137 kilometres or 85 miles) north of the equator, off the southern tip of the Malay Peninsula, bordering the Strait of Malacca to the west, the Singapore Strait to the south, the South China Sea to the east, and the Straits of Johor to the north. The country's territory comprises one main island, 63 satellite islands and islets, and one outlying islet; the combined area of these has increased by approximately 25% since the country's independence as a result of extensive land reclamation projects. It has the second highest population density of any country in the world, although there are numerous green and recreational spaces as a result of urban planning. With a multicultural population and in recognition of the cultural identities of the major ethnic groups within the nation, Singapore has four official languages: English, Malay, Mandarin, and Tamil. English is the lingua franca, with its exclusive use in numerous public services. Multi-racialism is enshrined in the constitution and continues to shape national policies in education, housing, and politics.. Singapore's history dates back at least eight hundred years, having been a maritime emporium known as Temasek and subsequently a major constituent part of several successive thalassocratic empires. Its contemporary era began in 1819, when Stamford Raffles established Singapore as an entrepôt trading post of the British Empire. In 1867, the colonies in Southeast Asia were reorganised, and Singapore came under the direct control of Britain as part of the Straits Settlements. During World War II, Singapore was occupied by Japan in 1942 and returned to British control as a separate Crown colony following Japan's surrender in 1945. Singapore gained self-governance in 1959\",\n",
       "  'Query': 'What is the population of Singapore'},\n",
       " {'Reasoning': ' According to the context, the population of Singapore is approximately 5.68 million people, as of 2020.',\n",
       "  'Feedback': 'FEEDBACK: No HINT'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critque_gen.traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792d6c8-5362-402b-85a0-146ec2a20dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
